name: 'Sitemap Crawler and Content Ingestion'

on:
  schedule:
    # Run daily at 3:00 AM UTC
    - cron: '0 3 * * *'
  # Allow manual triggering
  workflow_dispatch:

# Grant permissions for the job to write to the repository (for committing metrics)
permissions:
  contents: write

jobs:
  crawl-and-ingest:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip' # Enable caching for pip dependencies
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          # The main requirements.txt should include all necessary packages.
          # If not, add them here, e.g., pip install psycopg2-binary requests beautifulsoup4

      - name: Run Crawler and Ingestion Script
        id: crawler
        env:
          # Pass secrets to the Python script as environment variables
          DB_HOST: ${{ secrets.DB_HOST }}
          DB_PORT: ${{ secrets.DB_PORT }}
          DB_NAME: ${{ secrets.DB_NAME }}
          DB_USERNAME: ${{ secrets.DB_USERNAME }}
          DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          TOGETHER_API_KEY: ${{ secrets.TOGETHER_API_KEY }} 
        run: |
          # The Python script now handles all logic and logging.
          # We just run it and pipe its output to a log file.
          echo "::group::Crawler Log Output"
          python -m hubert.data_ingestion.huber_crawler.main | tee crawler_output.txt
          echo "::endgroup::"

      - name: Run Content Extraction
        env:
          DB_HOST: ${{ secrets.DB_HOST }}
          DB_PORT: ${{ secrets.DB_PORT }}
          DB_NAME: ${{ secrets.DB_NAME }}
          DB_USERNAME: ${{ secrets.DB_USERNAME }}
          DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
        run: |
          echo "::group::Content Extraction Log"
          python -m hubert.data_ingestion.huber_crawler.content_extractor | tee content_extractor_output.txt
          echo "::endgroup::"

      - name: Upload logs and metrics as artifacts
        if: always() # Run this step even if the crawler fails
        uses: actions/upload-artifact@v4
        with:
          name: crawler-run-logs-${{ github.run_id }}
                      path: |
              crawler_output.txt
              content_extractor_output.txt
              crawler_metrics.json
          retention-days: 7 # Keep logs for 7 days

      - name: Create job summary from metrics file
        if: success() && steps.crawler.outcome == 'success'
        run: |
          # Check if metrics file exists before trying to read it
          if [ ! -f "crawler_metrics.json" ]; then
            echo "## Sitemap Crawler Run Summary" >> $GITHUB_STEP_SUMMARY
            echo "âš ï¸ **Warning**: No metrics file was generated by the crawler." >> $GITHUB_STEP_SUMMARY
            echo "This suggests the crawler may have failed silently or encountered an error before completion." >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Run completed at: $(date)" >> $GITHUB_STEP_SUMMARY
            echo "**Status**: Failed - No metrics available" >> $GITHUB_STEP_SUMMARY
            exit 1
          fi
          
          # Read metrics directly from the JSON file to create a clean summary
          TOTAL_RUNTIME=$(jq -r '.total_runtime_seconds // 0' crawler_metrics.json)
          TOTAL_URLS=$(jq -r '.total_urls_found // 0' crawler_metrics.json)
          NEW_URLS=$(jq -r '.new_urls // 0' crawler_metrics.json)
          REMOVED_URLS=$(jq -r '.removed_urls // 0' crawler_metrics.json)
          ERRORS=$(jq -r '.errors // 0' crawler_metrics.json)

          echo "## Sitemap Crawler Run Summary" >> $GITHUB_STEP_SUMMARY
          echo "Run completed at: $(date)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|---|---|" >> $GITHUB_STEP_SUMMARY
          echo "| Total runtime | **${TOTAL_RUNTIME%.*} seconds** |" >> $GITHUB_STEP_SUMMARY
          echo "| Total URLs found | **$TOTAL_URLS** |" >> $GITHUB_STEP_SUMMARY
          echo "| New/Updated URLs | **$NEW_URLS** |" >> $GITHUB_STEP_SUMMARY
          echo "| Removed URLs | **$REMOVED_URLS** |" >> $GITHUB_STEP_SUMMARY
          echo "| Errors | **$ERRORS** |" >> $GITHUB_STEP_SUMMARY
          
      - name: Commit metrics history
        if: success()
        run: |
          # Create a directory for today's metrics
          METRICS_DIR="metrics/$(date +'%Y-%m-%d')"
          mkdir -p $METRICS_DIR
          
          # Check if files exist before trying to move them
          if [ -f "crawler_metrics.json" ]; then
            mv crawler_metrics.json $METRICS_DIR/
          else
            echo "âš ï¸ Warning: crawler_metrics.json not found, creating empty placeholder"
            echo '{"error": "No metrics file generated", "timestamp": "'$(date -u +"%Y-%m-%dT%H:%M:%S.%6NZ")'"}' > $METRICS_DIR/crawler_metrics.json
          fi
          
          if [ -f "crawler_output.txt" ]; then
            mv crawler_output.txt $METRICS_DIR/
          else
            echo "âš ï¸ Warning: crawler_output.txt not found, creating empty placeholder"
            echo "No crawler output generated on $(date)" > $METRICS_DIR/crawler_output.txt
          fi

          if [ -f "content_extractor_output.txt" ]; then
            mv content_extractor_output.txt $METRICS_DIR/
          else
            echo "âš ï¸ Warning: content_extractor_output.txt not found, creating empty placeholder"
            echo "No content extractor output generated on $(date)" > $METRICS_DIR/content_extractor_output.txt
          fi

          # Update historical metrics file
          if [ ! -f "metrics_history.csv" ]; then
            echo "date,total_runtime,total_urls,new_urls,removed_urls,errors" > metrics_history.csv
          fi
          
          # Append today's summary metrics
          TODAY=$(date +"%Y-%m-%d")
          if [ -f "$METRICS_DIR/crawler_metrics.json" ] && [ "$(jq -r '.error // ""' $METRICS_DIR/crawler_metrics.json)" = "" ]; then
            RUNTIME=$(jq -r '.total_runtime_seconds' $METRICS_DIR/crawler_metrics.json)
            URLS=$(jq -r '.total_urls_found' $METRICS_DIR/crawler_metrics.json)
            NEW=$(jq -r '.new_urls' $METRICS_DIR/crawler_metrics.json)
            REMOVED=$(jq -r '.removed_urls' $METRICS_DIR/crawler_metrics.json)
            ERRORS=$(jq -r '.errors' $METRICS_DIR/crawler_metrics.json)
            echo "$TODAY,${RUNTIME},${URLS},${NEW},${REMOVED},${ERRORS}" >> metrics_history.csv
          else
            echo "$TODAY,0,0,0,0,1" >> metrics_history.csv
          fi

          # Commit the changes back to the repository
          git config --global user.name 'GitHub Actions Bot'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          
          # Pull the latest changes before committing
          git pull
          
          git add metrics/ metrics_history.csv
          # Only commit and push if there are actual changes
          git diff --quiet && git diff --staged --quiet || (git commit -m "ðŸ“Š Update crawler metrics for $TODAY" && git push)
